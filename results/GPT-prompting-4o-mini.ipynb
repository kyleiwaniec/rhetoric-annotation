{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def77e51-ee06-49bf-9789-b03f88b8b83b",
   "metadata": {},
   "source": [
    "# Re-prompting GPT - V3\n",
    "The results from these are listed in the FT column in the paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b14a2-df1e-4537-9a46-0afa4eb63ec1",
   "metadata": {},
   "source": [
    "### Conda environment: transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7803f73-80b3-40fe-bd65-161ff02f9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /Users/kylehamilton/opt/anaconda3\n",
      "annotation               /Users/kylehamilton/opt/anaconda3/envs/annotation\n",
      "mapping                  /Users/kylehamilton/opt/anaconda3/envs/mapping\n",
      "mlflow                   /Users/kylehamilton/opt/anaconda3/envs/mlflow\n",
      "nlp                      /Users/kylehamilton/opt/anaconda3/envs/nlp\n",
      "pyg                      /Users/kylehamilton/opt/anaconda3/envs/pyg\n",
      "si                       /Users/kylehamilton/opt/anaconda3/envs/si\n",
      "torch                    /Users/kylehamilton/opt/anaconda3/envs/torch\n",
      "transformers          *  /Users/kylehamilton/opt/anaconda3/envs/transformers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b4fbd1-2897-4957-aaa8-def8559d179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import features\n",
    "import ast\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import backoff\n",
    "import logging\n",
    "import requests\n",
    "import re\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "import GPT_V2\n",
    "import GPT_V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b309e526-10d9-4534-a675-54ed46b0d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e19d4b-4e3f-4d51-a526-7988b79a8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = list(features.f_od.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53c60046-cd91-4350-80e6-9d374c70b176",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aspect',\n",
       " 'Emphasis',\n",
       " 'Figures_of_argument',\n",
       " 'Figures_of_word_choice',\n",
       " 'Language_of_origin',\n",
       " 'Language_varieties',\n",
       " 'Lexical_and_semantic_fields',\n",
       " 'Modifying_clauses',\n",
       " 'Modifying_phrases',\n",
       " 'Mood',\n",
       " 'New_words_and_changing_uses',\n",
       " 'Parallelism',\n",
       " 'Phrases_built_on_nouns',\n",
       " 'Phrases_built_on_verbs',\n",
       " 'Predication',\n",
       " 'Prosody_and_punctuation',\n",
       " 'Sentence_architecture',\n",
       " 'Series',\n",
       " 'Subject_choices',\n",
       " 'Tense',\n",
       " 'Tropes',\n",
       " 'Verb_choices']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "425da766-1346-4556-9120-fcf7df4dec42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = ['Aspect',\n",
    " 'Emphasis',\n",
    " 'Figures_of_argument',\n",
    " 'Figures_of_word_choice',\n",
    " 'Language_varieties',\n",
    " 'Lexical_and_semantic_fields',\n",
    " 'Modifying_clauses',\n",
    " 'Mood',\n",
    " 'New_words_and_changing_uses',\n",
    " 'Parallelism',\n",
    " 'Phrases_built_on_nouns',\n",
    " 'Phrases_built_on_verbs',\n",
    " 'Predication',\n",
    " 'Sentence_architecture',\n",
    " 'Series',\n",
    " 'Subject_choices',\n",
    " 'Tense',\n",
    " 'Tropes',\n",
    " 'Verb_choices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25516682-06bf-41d7-a6a7-e07a4f275a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo-1106 models\n",
    "\n",
    "models_dict = {\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kHQUVJr\":\"Aspect\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kJH3MlY\":\"Emphasis\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kJNlyTL\":\"Figures_of_argument\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kJHdiWj\":\"Figures_of_word_choice\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kSbYB5Z\":\"Language_varieties\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kSXEMmV\":\"Lexical_and_semantic_fields\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kSZGelp\":\"Modifying_clauses\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8krH6MwS\":\"Mood\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kTWojhH\":\"New_words_and_changing_uses\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kTceGAO\":\"Parallelism\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kVWQWJW\":\"Phrases_built_on_nouns\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kVFi1Q9\":\"Phrases_built_on_verbs\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kVdlK5k\":\"Predication\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kYKKQWJ\":\"Sentence_architecture\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kYKGlP0\":\"Series\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kYdJ0eq\":\"Subject_choices\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8ka5hSbD\":\"Tense\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kaunpHX\":\"Tropes\",\n",
    "\"ft:gpt-3.5-turbo-1106:personal::8kZhtRxY\":\"Verb_choices\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc38e5c3-10f9-4e0d-8e8f-30f7f69a2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini models\n",
    "\n",
    "models_dict = {\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:aspect:9obT66PC\":\"Aspect\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:emphasis:9obaWiHn\":\"Emphasis\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:fig-of-argument:9oc4TdDq\":\"Figures_of_argument\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:fig-of-wordchoice:9ocKMI6k\":\"Figures_of_word_choice\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal::9ockWPGZ\":\"Language_varieties\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:lex-sem-fields:9otZKXdw\":\"Lexical_and_semantic_fields\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:mod-clauses:9oy9tJ5H\":\"Modifying_clauses\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:mood:9oyFktVS\":\"Mood\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:new-words:9ozqBKSc\":\"New_words_and_changing_uses\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:parallel:9ozuSX9K\":\"Parallelism\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:phrases-nouns:9p1UUoyi\":\"Phrases_built_on_nouns\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:phrases-verbs:9pGaHXJK\":\"Phrases_built_on_verbs\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:predication:9pKoOFeY\":\"Predication\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:sent-architecture:9pKjV2LY\":\"Sentence_architecture\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:series:9pO28rQJ\":\"Series\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:tense:9pO6FmBe\":\"Tense\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:subject-choices:9oWoIqLH\":\"Subject_choices\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:tropes:9pagEI0v\":\"Tropes\",\n",
    "\"ft:gpt-4o-mini-2024-07-18:personal:verb-choices:9pdQlxYi\":\"Verb_choices\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2db66621-b9b1-4bbf-ba5e-ed8bb8f8e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluating models trained on different features\n",
    "alt = \"_FT_Maj\"\n",
    "alt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89890003-9bdb-4c94-9187-5a13c77c498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = ['Subject_choices']\n",
    "MODEL = \"gpt-4o\" #\"ft:gpt-4o-mini-2024-07-18:personal:subject-choices:9oWoIqLH\", \"gpt-4o-mini-2024-07-18\"\n",
    "model_version = \"_FT_Maj_gpt-4o-mini\"\n",
    "version = \"V4\"\n",
    "output_path = \"data/\"+version+\"/\"+model_version+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33ebe0a8-90a9-45c0-b426-66e2f083ff3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_gpt_response\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(GPT_V3)\n",
    "gpt = GPT_V3.GPT(MODEL)\n",
    "\n",
    "def parseRes(x,_property):\n",
    "    try:\n",
    "        result = gpt.parseYNResponse(x,_property)\n",
    "    except():\n",
    "        result = []\n",
    "    return result\n",
    "\n",
    "\n",
    "def fixProperties(s,feature):\n",
    "    new_list = []\n",
    "    if type(s) == str:\n",
    "        s = ast.literal_eval(s) \n",
    "\n",
    "    for l in s:\n",
    "        new_list.append(gpt.mapToProperty(l,feature))\n",
    "    return new_list\n",
    "\n",
    "\n",
    "\n",
    "def run(data,FEATURE,temp,version,model_version,MODEL):\n",
    "    temp = str(temp)\n",
    "    \n",
    "    responses_data = []\n",
    "    gpt = GPT_V3.GPT(MODEL)\n",
    "\n",
    "    for row in tqdm(data.iterrows()):\n",
    "        sentence = row[1]['text']\n",
    "        feature = row[1]['feature_id']\n",
    "        sid = row[1]['sentence_id']\n",
    "        \n",
    "        responses = gpt.get_gpt_response(sentence,feature,sid,float(temp),model_version)\n",
    "        \n",
    "        for res in responses:\n",
    "            responses_data.append([sid, res[1], res[3]])\n",
    "\n",
    "    df = pd.DataFrame(responses_data, columns=['sentence_id','property'+model_version,'res'+model_version+'_'+temp+'_'+version])\n",
    "     \n",
    "    data = data.merge(df, how='outer',on='sentence_id')\n",
    "    data.to_csv(output_path+version+\"_\"+FEATURE+alt+\".csv\",index=None)\n",
    "    \n",
    "    data['property'+model_version+'_'+temp+'_'+version] = \\\n",
    "        data.apply(lambda row: parseRes(row['res'+model_version+'_'+temp+'_'+version],row['property'+model_version]),axis=1) \n",
    "    \n",
    "    data.to_csv(output_path+version+\"_\"+FEATURE+alt+\".csv\",index=None)\n",
    "    \n",
    "    print(f\"There were {len(gpt.errors)} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4fcd1de7-9bfd-4674-bf5a-5be4430f96ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tropes _FT_Maj_gpt-4o-mini V4 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [45:46, 85.81s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/g1t9ds3s01z05zs4qg2dngk80000gn/T/ipykernel_29131/371554619.py:35: FutureWarning: The provided callable <built-in function sum> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  df = df.groupby(['sentence_id']).agg({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "====================================================================================================\n",
      "Verb_choices _FT_Maj_gpt-4o-mini V4 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [16:41, 33.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 errors.\n",
      "0.7666666666666667\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/vt/g1t9ds3s01z05zs4qg2dngk80000gn/T/ipykernel_29131/371554619.py:35: FutureWarning: The provided callable <built-in function sum> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  df = df.groupby(['sentence_id']).agg({\n"
     ]
    }
   ],
   "source": [
    "# ADDS THE PROPERTY NAME TO THE RESPONSE OBJECT FOR EASIER READING LATER.\n",
    "def combine(prop, res):\n",
    "    # print(res)\n",
    "    _json_obj = gpt.responseToJson(res)\n",
    "    _json_obj['Property'] = prop\n",
    "    \n",
    "    return _json_obj\n",
    "\n",
    "def removeErrors(s):\n",
    "    if \"parse error\" in s or \"timeout\" in s:\n",
    "        s = \"[]\"\n",
    "        \n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "for k, v in models_dict.items():\n",
    "\n",
    "    FEATURE = v\n",
    "    MODEL = k\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(\"data/human_gpt_verified/\"+FEATURE+\".csv\")\n",
    "    df = df[df[\"humans isCorrect\"]>=0]\n",
    "\n",
    "    # df = df[:2]\n",
    "\n",
    "    print(FEATURE, model_version, version, len(df))\n",
    "    run(df,FEATURE,0.0,version,model_version,MODEL)\n",
    "\n",
    "    # Combine\n",
    "    df = pd.read_csv(output_path+\"/\"+version+\"_\"+FEATURE+alt+\".csv\")\n",
    "    df['property'+model_version+'_0.0_'+version] = df['property'+model_version+'_0.0_'+version].apply(removeErrors)\n",
    "    df['res'+model_version+'_0.0_'+version] = df.apply(lambda x: combine(x['property'+model_version],x['res'+model_version+'_0.0_'+version]), axis=1)\n",
    "    df['sentence_id'] = df['sentence_id'].apply(lambda x: int(x))\n",
    "\n",
    "    df = df.groupby(['sentence_id']).agg({\n",
    "        'sentence_id':lambda x: x.iloc[0], \n",
    "        'technique':lambda x: x.iloc[0], \n",
    "        'text':lambda x: x.iloc[0], \n",
    "        'feature_id':lambda x: x.iloc[0], \n",
    "        'props_a20':lambda x: x.iloc[0],\n",
    "        'props_a21':lambda x: x.iloc[0], \n",
    "        'props_a22':lambda x: x.iloc[0], \n",
    "        'annotator_consistency':lambda x: x.iloc[0],\n",
    "        'props_gpt4_majority':lambda x: x.iloc[0], \n",
    "        'res_1.0_1':lambda x: x.iloc[0], \n",
    "        'gpt_props_1.0_1':lambda x: x.iloc[0], \n",
    "        'res_1.0_2':lambda x: x.iloc[0],\n",
    "        'gpt_props_1.0_2':lambda x: x.iloc[0], \n",
    "        'res_1.0_3':lambda x: x.iloc[0], \n",
    "        'gpt_props_1.0_3':lambda x: x.iloc[0],\n",
    "        'gpt3.5_1.0_consistency':lambda x: x.iloc[0], \n",
    "        'res_0.2_1':lambda x: x.iloc[0], \n",
    "        'gpt_props_0.2_1':lambda x: x.iloc[0], \n",
    "        'res_0.2_2':lambda x: x.iloc[0],\n",
    "        'gpt_props_0.2_2':lambda x: x.iloc[0], \n",
    "        'res_0.2_3':lambda x: x.iloc[0], \n",
    "        'gpt_props_0.2_3':lambda x: x.iloc[0],\n",
    "        'gpt3.5_0.2_consistency':lambda x: x.iloc[0], \n",
    "        'gpt3.5_0.2_majority':lambda x: x.iloc[0], \n",
    "        'humans isCorrect':lambda x: x.iloc[0],\n",
    "        'gpt isCorrect':lambda x: x.iloc[0], \n",
    "        'comments':lambda x: x.iloc[0], \n",
    "        'ground truth':lambda x: x.iloc[0],\n",
    "        'property'+model_version:list, \n",
    "        'res'+model_version+'_0.0_'+version:list,\n",
    "        'property'+model_version+'_0.0_'+version:sum\n",
    "    })\n",
    "    df=df.drop('property'+model_version,axis=1)\n",
    "    df.to_csv(output_path+\"/_\"+version+\"_\"+FEATURE+alt+\".csv\",index=None)\n",
    "    df=pd.read_csv(output_path+\"/_\"+version+\"_\"+FEATURE+alt+\".csv\")\n",
    "    df = df[df[\"humans isCorrect\"]>=0]\n",
    "    df['agreement'] = df.apply(lambda x: utils.calcAgreement(x[\"ground truth\"],x[\"property\"+model_version+\"_0.0_\"+version]), axis=1)\n",
    "    \n",
    "    print(Counter(df['agreement'])[1]/len(df))\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86ebace6-cfcd-4509-8b85-15973a0664c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "_FT_Maj_gpt-4o-mini\n",
    "\n",
    "Aspect _FT_Maj_gpt-4o-mini V4 36\n",
    "36it [02:20,  3.90s/it]\n",
    "There were 0 errors.\n",
    "0.5277777777777778\n",
    "====================================================================================================\n",
    "Emphasis _FT_Maj_gpt-4o-mini V4 15\n",
    "15it [05:20, 21.34s/it]\n",
    "There were 0 errors.\n",
    "0.13333333333333333\n",
    "====================================================================================================\n",
    "Figures_of_argument _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [10:49, 21.64s/it]\n",
    "There were 0 errors.\n",
    "0.4666666666666667\n",
    "====================================================================================================\n",
    "Figures_of_word_choice _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [08:14, 16.48s/it]\n",
    "There were 0 errors.\n",
    "0.43333333333333335\n",
    "====================================================================================================\n",
    "Language_varieties _FT_Maj_gpt-4o-mini V4 46\n",
    "46it [1:09:53, 91.17s/it] \n",
    "There were 0 errors.\n",
    "0.6304347826086957\n",
    "====================================================================================================\n",
    "Lexical_and_semantic_fields _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [04:16,  8.56s/it]\n",
    "There were 0 errors.\n",
    "0.06666666666666667\n",
    "====================================================================================================\n",
    "Modifying_clauses _FT_Maj_gpt-4o-mini V4 29\n",
    "29it [03:23,  7.02s/it]\n",
    "There were 0 errors.\n",
    "0.6551724137931034\n",
    "====================================================================================================\n",
    "Mood _FT_Maj_gpt-4o-mini V4 29\n",
    "29it [02:58,  6.17s/it]\n",
    "There were 0 errors.\n",
    "0.20689655172413793\n",
    "====================================================================================================\n",
    "New_words_and_changing_uses _FT_Maj_gpt-4o-mini V4 25\n",
    "25it [09:14, 22.20s/it]\n",
    "There were 0 errors.\n",
    "0.56\n",
    "====================================================================================================\n",
    "Parallelism _FT_Maj_gpt-4o-mini V4 31\n",
    "31it [02:00,  3.88s/it]\n",
    "There were 0 errors.\n",
    "0.8387096774193549\n",
    "====================================================================================================\n",
    "Phrases_built_on_nouns _FT_Maj_gpt-4o-mini V4 31\n",
    "31it [02:00,  3.90s/it]\n",
    "There were 0 errors.\n",
    "0.9032258064516129\n",
    "====================================================================================================\n",
    "Phrases_built_on_verbs _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [01:05,  2.18s/it]\n",
    "There were 0 errors.\n",
    "0.9666666666666667\n",
    "====================================================================================================\n",
    "Predication _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [01:20,  2.69s/it]\n",
    "There were 0 errors.\n",
    "0.6333333333333333\n",
    "====================================================================================================\n",
    "Sentence_architecture _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [01:43,  3.46s/it]\n",
    "There were 0 errors.\n",
    "0.6\n",
    "====================================================================================================\n",
    "Series _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [02:30,  5.00s/it]\n",
    "There were 0 errors.\n",
    "0.8333333333333334\n",
    "====================================================================================================\n",
    "Tense _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [01:30,  3.02s/it]\n",
    "There were 0 errors.\n",
    "0.5666666666666667\n",
    "====================================================================================================\n",
    "Tropes _FT_Maj_gpt-4o-mini V4 32\n",
    "32it [45:46, 85.81s/it] \n",
    "There were 0 errors.\n",
    "0.5\n",
    "====================================================================================================\n",
    "Verb_choices _FT_Maj_gpt-4o-mini V4 30\n",
    "30it [16:41, 33.37s/it]\n",
    "There were 0 errors.\n",
    "0.7666666666666667"
   ]
  },
  {
   "cell_type": "raw",
   "id": "713d2459-eccf-47b8-8dbf-026321cd90c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "GPT-4o\n",
    "\n",
    "Aspect gpt-4o V4 36\n",
    "36it [06:12, 10.34s/it]\n",
    "There were 0 errors.\n",
    "0.5\n",
    "====================================================================================================\n",
    "Emphasis gpt-4o V4 15\n",
    "15it [02:41, 10.74s/it]\n",
    "There were 0 errors.\n",
    "0.3333333333333333\n",
    "====================================================================================================\n",
    "Figures_of_argument gpt-4o V4 30\n",
    "30it [08:19, 16.64s/it]\n",
    "There were 0 errors.\n",
    "0.5333333333333333\n",
    "====================================================================================================\n",
    "Figures_of_word_choice gpt-4o V4 30\n",
    "30it [08:51, 17.71s/it]\n",
    "There were 0 errors.\n",
    "0.1\n",
    "====================================================================================================\n",
    "Language_varieties gpt-4o V4 46\n",
    "46it [18:14, 23.80s/it]\n",
    "There were 0 errors.\n",
    "0.45652173913043476\n",
    "====================================================================================================\n",
    "Lexical_and_semantic_fields gpt-4o V4 30\n",
    "30it [06:10, 12.36s/it]\n",
    "There were 0 errors.\n",
    "0.4\n",
    "====================================================================================================\n",
    "Modifying_clauses gpt-4o V4 29\n",
    "29it [05:29, 11.35s/it]\n",
    "There were 0 errors.\n",
    "0.5862068965517241\n",
    "====================================================================================================\n",
    "Mood gpt-4o V4 29\n",
    "29it [06:40, 13.80s/it]\n",
    "There were 0 errors.\n",
    "0.6206896551724138\n",
    "====================================================================================================\n",
    "New_words_and_changing_uses gpt-4o V4 25\n",
    "25it [11:52, 28.48s/it]\n",
    "There were 0 errors.\n",
    "0.28\n",
    "====================================================================================================\n",
    "Parallelism gpt-4o V4 31\n",
    "31it [04:40,  9.03s/it]\n",
    "There were 0 errors.\n",
    "0.8387096774193549\n",
    "====================================================================================================\n",
    "Phrases_built_on_nouns gpt-4o V4 31\n",
    "31it [04:26,  8.60s/it]\n",
    "There were 0 errors.\n",
    "0.7419354838709677\n",
    "====================================================================================================\n",
    "Phrases_built_on_verbs gpt-4o V4 30\n",
    "30it [01:56,  3.90s/it]\n",
    "There were 0 errors.\n",
    "0.6333333333333333\n",
    "====================================================================================================\n",
    "Predication gpt-4o V4 30\n",
    "30it [03:23,  6.80s/it]\n",
    "There were 0 errors.\n",
    "0.16666666666666666\n",
    "====================================================================================================\n",
    "Sentence_architecture gpt-4o V4 30\n",
    "30it [05:13, 10.44s/it]\n",
    "There were 0 errors.\n",
    "0.8\n",
    "====================================================================================================\n",
    "Series gpt-4o V4 30\n",
    "30it [06:10, 12.36s/it]\n",
    "There were 0 errors.\n",
    "0.9\n",
    "====================================================================================================\n",
    "Subject_choices gpt-4o V4 30\n",
    "30it [05:53, 11.78s/it]\n",
    "There were 0 errors.\n",
    "0.43333333333333335\n",
    "====================================================================================================\n",
    "Tense gpt-4o V4 30\n",
    "30it [03:41,  7.37s/it]\n",
    "There were 0 errors.\n",
    "0.5\n",
    "====================================================================================================\n",
    "Tropes gpt-4o V4 32\n",
    "32it [15:31, 29.10s/it]\n",
    "There were 0 errors.\n",
    "0.375\n",
    "====================================================================================================\n",
    "Verb_choices gpt-4o V4 30\n",
    "30it [05:42, 11.43s/it]\n",
    "There were 0 errors.\n",
    "0.6666666666666666"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee434807-3fd1-4ebf-b651-fc0ebf367e45",
   "metadata": {
    "tags": []
   },
   "source": [
    "GPT-4o-mini\n",
    "\n",
    "\n",
    "Aspect gpt-4o-mini V4 36\n",
    "36it [04:22,  7.29s/it]\n",
    "There were 0 errors.\n",
    "0.4166666666666667\n",
    "====================================================================================================\n",
    "Emphasis gpt-4o-mini V4 15\n",
    "15it [01:32,  6.15s/it]\n",
    "There were 0 errors.\n",
    "0.26666666666666666\n",
    "====================================================================================================\n",
    "Figures_of_argument gpt-4o-mini V4 30\n",
    "30it [05:09, 10.33s/it]\n",
    "There were 0 errors.\n",
    "0.4666666666666667\n",
    "====================================================================================================\n",
    "Figures_of_word_choice gpt-4o-mini V4 30\n",
    "30it [06:55, 13.84s/it]\n",
    "There were 0 errors.\n",
    "0.2\n",
    "====================================================================================================\n",
    "Language_varieties gpt-4o-mini V4 46\n",
    "46it [12:30, 16.31s/it]\n",
    "There were 0 errors.\n",
    "0.43478260869565216\n",
    "====================================================================================================\n",
    "Lexical_and_semantic_fields gpt-4o-mini V4 30\n",
    "30it [04:14,  8.48s/it]\n",
    "There were 0 errors.\n",
    "0.16666666666666666\n",
    "====================================================================================================\n",
    "Modifying_clauses gpt-4o-mini V4 29\n",
    "29it [03:37,  7.51s/it]\n",
    "There were 0 errors.\n",
    "0.6206896551724138\n",
    "====================================================================================================\n",
    "Modifying_phrases gpt-4o-mini V4 4\n",
    "4it [00:22,  5.66s/it]\n",
    "There were 0 errors.\n",
    "0.0\n",
    "====================================================================================================\n",
    "Mood gpt-4o-mini V4 29\n",
    "29it [04:11,  8.67s/it]\n",
    "There were 0 errors.\n",
    "0.4827586206896552\n",
    "====================================================================================================\n",
    "New_words_and_changing_uses gpt-4o-mini V4 25\n",
    "25it [08:28, 20.36s/it]\n",
    "There were 0 errors.\n",
    "0.28\n",
    "====================================================================================================\n",
    "Parallelism gpt-4o-mini V4 31\n",
    "31it [03:07,  6.06s/it]\n",
    "There were 0 errors.\n",
    "0.8387096774193549\n",
    "====================================================================================================\n",
    "Phrases_built_on_nouns gpt-4o-mini V4 31\n",
    "31it [03:15,  6.29s/it]\n",
    "There were 0 errors.\n",
    "0.9032258064516129\n",
    "====================================================================================================\n",
    "Phrases_built_on_verbs gpt-4o-mini V4 30\n",
    "30it [01:38,  3.28s/it]\n",
    "There were 0 errors.\n",
    "0.6666666666666666\n",
    "====================================================================================================\n",
    "Predication gpt-4o-mini V4 30\n",
    "30it [02:20,  4.67s/it]\n",
    "There were 0 errors.\n",
    "0.13333333333333333\n",
    "====================================================================================================\n",
    "Sentence_architecture gpt-4o-mini V4 30\n",
    "30it [04:02,  8.10s/it]\n",
    "There were 0 errors.\n",
    "0.6333333333333333\n",
    "====================================================================================================\n",
    "Series gpt-4o-mini V4 30\n",
    "30it [03:38,  7.28s/it]\n",
    "There were 0 errors.\n",
    "0.8666666666666667\n",
    "====================================================================================================\n",
    "Subject_choices gpt-4o-mini V4 30\n",
    "30it [05:06, 10.21s/it]\n",
    "There were 0 errors.\n",
    "0.4666666666666667\n",
    "====================================================================================================\n",
    "Tense gpt-4o-mini V4 30\n",
    "30it [03:14,  6.48s/it]\n",
    "There were 0 errors.\n",
    "0.5\n",
    "====================================================================================================\n",
    "Tropes gpt-4o-mini V4 30\n",
    "0.4375\n",
    "====================================================================================================\n",
    "Verb_choices gpt-4o-mini V4 30\n",
    "30it [03:51,  7.70s/it]\n",
    "There were 0 errors.\n",
    "0.3333333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe9402-d935-4527-94eb-ef209fbac22d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get the accuracy scores from fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4e4c7-54f1-4465-81ef-b78823770aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "model_version = \"_FT_Maj_gpt-4o-mini\"\n",
    "version = \"V4\"\n",
    "output_path = \"data/\"+version+\"/\"+model_version+\"/\"\n",
    "\n",
    "\n",
    "for k,v in models_dict.items():\n",
    "    FEATURE = v\n",
    "    MODEL = k\n",
    "\n",
    "    if MODEL != \"\":\n",
    "\n",
    "        df=pd.read_csv(output_path+\"/_\"+version+\"_\"+FEATURE+alt+\".csv\")\n",
    "        df['agreement'] = df.apply(lambda x: utils.calcAgreement(x[\"ground truth\"],x[\"property\"+model_version+\"_0.0_\"+version]), axis=1)\n",
    "\n",
    "        print(v, model_version, version)\n",
    "        print(Counter(df['agreement'])[1]/len(df))\n",
    "        print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ee319-2a30-4513-876d-3fb27ff79a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13e71401-6d24-45bb-a2c5-ed367017ab72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_FT_Maj_gpt-4o-mini V4\n",
      "0.26666666666666666\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_version = \"_FT_Maj_gpt-4o-mini\"\n",
    "version = \"V4\"\n",
    "\n",
    "df=pd.read_csv(\"data/V4/_FT_Maj_gpt-4o-mini/_V4_Subject_choices.csv\")\n",
    "df['agreement'] = df.apply(lambda x: utils.calcAgreement(x[\"ground truth\"],x[\"property\"+model_version+\"_0.0_\"+version]), axis=1)\n",
    "\n",
    "print(model_version, version)\n",
    "print(Counter(df['agreement'])[1]/len(df))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9bb20-b9a2-44dc-9601-38e57abff2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fc6ca7d8-7f0a-4396-8a0b-4cd52cac158e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a6193-8ea0-4db2-97f5-4a8fddd836fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08296d81-d107-4878-9a97-eab0dc89f984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
