{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a5638e-2641-4189-a678-84c749340aa7",
   "metadata": {},
   "source": [
    "# Annotation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fec8c3-c8fa-4f3d-8c1b-427b77bdb82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pymysql\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import utils\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845c028-7582-45e0-a63d-48bd1add3f6b",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9393577-de8c-4da8-96b8-598902bb5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/human_gpt_verified/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b9920a2-f09d-40a3-8d7c-3823601d766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Aspect',\n",
    " 'Emphasis',\n",
    " 'Figures_of_argument',\n",
    " 'Figures_of_word_choice',\n",
    " 'Language_of_origin',\n",
    " 'Language_varieties',\n",
    " 'Lexical_and_semantic_fields',\n",
    " 'Modifying_clauses',\n",
    " 'Modifying_phrases',\n",
    " 'Mood',\n",
    " 'New_words_and_changing_uses',\n",
    " 'Parallelism',\n",
    " 'Phrases_built_on_nouns',\n",
    " 'Phrases_built_on_verbs',\n",
    " 'Predication',\n",
    " 'Prosody_and_punctuation',\n",
    " 'Sentence_architecture',\n",
    " 'Series',\n",
    " 'Subject_choices',\n",
    " 'Tense',\n",
    " 'Tropes',\n",
    " 'Verb_choices']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa5033fa-511c-4b94-bc95-1910236c4cff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#############################################\n",
    "# RUN ONCE - DO NOT OVERWRITE\n",
    "#############################################\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    df = pd.read_csv(data_path+feature+\".csv\")\n",
    "    # print(\"*\"*100)\n",
    "    print(\"# \", feature)\n",
    "    \n",
    "    if \"annotator consistency\" in df.columns:\n",
    "        df = df.rename(columns={\"annotator consistency\":\"annotator_consistency\"})\n",
    "    if \"annotator_consitency\" in df.columns:\n",
    "        df = df.rename(columns={\"annotator_consitency\":\"annotator_consistency\"})\n",
    "\n",
    "    print(\"### Number of rows where there is consistency\")\n",
    "    print(\"```\")\n",
    "    print(\"humans: \",sum(df[\"annotator_consistency\"]))\n",
    "    print(\"GPT: \",sum(df[\"gpt3.5_0.2_consistency\"]))\n",
    "    print(\"```\")\n",
    "    # calculate gpt majority\n",
    "    def calcMaj(_iter):\n",
    "        return utils.find_majority(_iter)[0]\n",
    "\n",
    "    df[\"gpt3.5_0.2_majority\"] = df.apply(lambda x: calcMaj([x['gpt_props_0.2_1'],\n",
    "                                                            x['gpt_props_0.2_2'],\n",
    "                                                            x['gpt_props_0.2_3']]),\n",
    "                                         axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    true_df = df[df[\"annotator_consistency\"]==True]\n",
    "\n",
    "    # rows where human annotator is true and gpt majority is different\n",
    "    # (gpt is not neccessarily consistent)\n",
    "    human_gpt_differ = true_df[true_df[\"props_a22\"] != true_df[\"gpt3.5_0.2_majority\"]]\n",
    "    print(\"### Number of examples to verify\")\n",
    "    print(\"```\")\n",
    "    print(len(human_gpt_differ))\n",
    "    print(\"```\")\n",
    "    # coverage of properties where humans agree\n",
    "    c_f = Counter(human_gpt_differ[\"props_a22\"])\n",
    "    print(\"### Coverage of properties where humans agree:\")\n",
    "    print(\"```\")\n",
    "    for k,v in c_f.items():\n",
    "        print(v,\"\\t\",k)\n",
    "    print(\"```\")\n",
    "\n",
    "    # human_gpt_differ.to_csv(\"data/human_gpt_verified/\"+feature+\".csv\",index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03c15762-bd92-4b6b-8c33-627cd99329ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# If there are less than 30 examples returned from the above function, sample another 30-n examples\n",
    "\n",
    "data_path = \"data/human_gpt_verified/\"\n",
    "data_path_supplementary = \"data/all_features_and_gpt/\"\n",
    "\n",
    "def calcMaj(_iter):\n",
    "        return utils.find_majority(_iter)[0]\n",
    "    \n",
    "    \n",
    "for feature in features:\n",
    "    df = pd.read_csv(data_path+feature+\".csv\")\n",
    "\n",
    "    df_non_empty = df[df[\"props_a22\"]!=\"[]\"]\n",
    "    \n",
    "    l = len(df_non_empty)\n",
    "    \n",
    "    if l < 30:\n",
    "        df_s = pd.read_csv(data_path_supplementary+feature+\".csv\")\n",
    "        sample = df_s.sample(30-l)\n",
    "\n",
    "        sample[\"gpt3.5_0.2_majority\"] = sample.apply(lambda x: calcMaj([x['gpt_props_0.2_1'],\n",
    "                                                                x['gpt_props_0.2_2'],\n",
    "                                                                x['gpt_props_0.2_3']]),\n",
    "                                             axis=1)\n",
    "\n",
    "        result = pd.concat([df,sample])\n",
    "        \n",
    "        print(feature, len(result))\n",
    "\n",
    "        result.to_csv(\"data/human_gpt_verified/\"+feature+\".csv\",index = False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d926f938-fdd0-4f9c-be0d-40d7b0506261",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# mood. get some examples which are not 'indicative'\n",
    "features = ['Mood']\n",
    "data_path = \"data/human_gpt_verified/\"\n",
    "data_path_supplementary = \"data/all_features_and_gpt/\"\n",
    "\n",
    "def calcMaj(_iter):\n",
    "        return utils.find_majority(_iter)[0]\n",
    "    \n",
    "    \n",
    "for feature in features:\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(data_path_supplementary+feature+\".csv\")\n",
    "    df_not_indicative = df[df['props_a22']!= \"['indicative']\"]\n",
    "    \n",
    "    l = len(df_not_indicative[df_not_indicative['annotator_consitency']=='TRUE'])\n",
    "\n",
    "    sample = df_not_indicative\n",
    "\n",
    "    sample[\"gpt3.5_0.2_majority\"] = sample.apply(lambda x: calcMaj([x['gpt_props_0.2_1'],\n",
    "                                                            x['gpt_props_0.2_2'],\n",
    "                                                            x['gpt_props_0.2_3']]),\n",
    "                                         axis=1)\n",
    "\n",
    "    sample.to_csv(\"data/human_gpt_verified/\"+feature+\"_02.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08bf76-74a6-4f9f-89a4-8e9d6db822df",
   "metadata": {},
   "source": [
    "# Manual error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8b170-a8dd-49d1-85d5-d24bfae64295",
   "metadata": {},
   "source": [
    "## Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7d915c0a-5417-48c4-8027-2b1026928065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "human accuracy: 0.9428571428571428\n",
      "gpt accuracy: 0.14285714285714285\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1 \t gpt inserted its own property\n",
      "1 \t gtp mistook adjective for verb (growing)\n",
      "1 \t the correct answer is simple  (email begins with) and progressinve (admitting)\n",
      "17 \t Seems to be relying on the definition provided in a looser way\n",
      "6 \t nan\n",
      "1 \t gpt missed the part of  sentence\n",
      "7 \t this should be an empty list\n",
      "1 \t explanation is just wrong\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Aspect.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "l = len(verified_df)\n",
    "print(l)\n",
    "print(\"human accuracy:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b346ec-c782-45e2-8b1b-068c995a9001",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "1. GPT is not using the definitions provided in the context of grammar. \n",
    "2. GPT often provides a property when no property is applicable.\n",
    "\n",
    "We only verified rows where gpt majority responses were different from human annotators. This does not preclude error 1 from occuring where there is agreement, only the agreement is by chance. This is particulalry true for the 'simple' property which is by far the majority property present in the Aspect feature.\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "* redesign the prompt to clarify that the answer sought is in the context of grammar.\n",
    "* verify again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb4398-a5d2-4bdb-8208-aea6367ebe19",
   "metadata": {},
   "source": [
    "# Emphahsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "31554e33-1b4d-40fb-943b-9378f5b9954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "human accuracy: 0.6\n",
      "gpt accuracy: 0.4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "4 \t GPT is misinterpreting the property. Using it out of context of grammar.\n",
      "2 \t Annotaors missed 'fron inversion'\n",
      "1 \t GPT classification and explanation both wrong\n",
      "8 \t nan\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Emphasis.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "print(\"human accuracy:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03cd06-f20c-4327-9669-c0f26122597c",
   "metadata": {},
   "source": [
    "It's hard to know the intended emphasis, so it's hard to know which property is being employed. I'm not sure this can be annotated without additional context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b1a4b-6461-4e8b-ba36-69bcfb063960",
   "metadata": {},
   "source": [
    "# Figures of Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "400f7bdf-7de2-4655-b21f-48939d4c8d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 0.8\n",
      "human accuracy all examples: 0.6666666666666666\n",
      "gpt accuracy all examples: 0.16666666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "4 \t GPT is misinterpreting the property. Using it out of context of grammar.\n",
      "2 \t Annotaors missed 'fron inversion'\n",
      "1 \t classification and explanation both wrong\n",
      "14 \t nan\n",
      "1 \t GPT is making stuff up\n",
      "7 \t GPT classification and explanation both wrong\n",
      "1 \t GPT is pertially correct\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Figures_of_argument.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5a340-cc7d-4ef0-8185-202cae6b2119",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "1. GPT incorrectly assigns 'antithesis' to almost all examples.\n",
    "2. GPT often provides a property when no property is applicable.\n",
    "\n",
    "### Next steps\n",
    "1. Instead of instructing GPT to return an empty list when no properties are applicable, create an explicit option of \"none of the above properties apply to the given sentence\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339ca82-1bf3-4224-afbd-16e89c47bf91",
   "metadata": {},
   "source": [
    "## V3\n",
    "* Figures of argument are classified individually\n",
    "* GPT3.5 Accuracy: 0.53\n",
    "* GPT4 Accuracy: 0.57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db20e4-162f-403c-a4db-4ccfbddaf082",
   "metadata": {},
   "source": [
    "# Figures of word choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "87bbab2f-467d-4262-9596-6a0b65a9a06c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 0.3\n",
      "human accuracy all examples: 0.1\n",
      "gpt accuracy all examples: 0.06666666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "17 \t nan\n",
      "8 \t GPT explanation doesn't reflect given sentence\n",
      "5 \t GPT is misinterpreting the property. Using it out of context of grammar.\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Figures_of_word_choice.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb2a42-e9d4-42bb-9f32-25611930beed",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "Emphasis is the most common property assignment by humans. However, this is not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02698603-6734-4a17-9e11-cd69afdfa952",
   "metadata": {},
   "source": [
    "# Language of origin\n",
    "This feature should be assigned using an etymological dictionary rather than human annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b0638-a9d4-48b8-a5ec-55200d026d58",
   "metadata": {},
   "source": [
    "# Language varieties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d140cc88-8760-4ebd-b4ee-84cdbeec95f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "human accuracy all examples: 0.7391304347826086\n",
      "gpt accuracy all examples: 0.010869565217391304\n",
      "----------------------------------------------------------------------------------------------------\n",
      "45 \t nan\n",
      "1 \t it's possible GPT picked up the word 'formally' and assinged this property on that basis rather than reasoning through the instructions.\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Language_varieties.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23afea-58dc-41c8-8627-671da0b982f8",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "The majority of cases should include 'correctness', 'clarity', and 'middle'. Since all of the sentences are taken from news articles, and the intended audience is the general public, this is expected.\n",
    "\n",
    "Often, GPT was partially correct by including one or more of these properties. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419bd6e6-020e-430d-a13f-c0506e9f34e1",
   "metadata": {},
   "source": [
    "# Lexical and semantic fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8b6d2a8b-7235-44e8-a424-38396a54b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 0.375\n",
      "human accuracy all examples: 0.1\n",
      "gpt accuracy all examples: 0.2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1 \t humans missed one\n",
      "29 \t nan\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Lexical_and_semantic_fields.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49d298-672a-425e-8775-66c82e827974",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "This one seems particulalry hard for both humans and GPT. The differences between the properties can be very subtle, and there is room for subjectivity. Ideally, this would be annotated and discussed to reach consensus. \n",
    "\n",
    "The GPT explanations sometimes re-word a property definition but assign a different property to that definition. It is mixing up the properties and their definitions. \n",
    "\n",
    "To improve GPT, we need to think about rewriting the instructions. We could also think about consolidating some of the properties. Examples would probably go a long way here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a31c6-519c-403a-8179-5910fec7595c",
   "metadata": {},
   "source": [
    "# Modifying clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a5d8f0e1-9959-4957-8887-e04a4a91de34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "human accuracy all examples: 0.6896551724137931\n",
      "gpt accuracy all examples: 0.10344827586206896\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1 \t gpt picked up the second clause but did not assing the right porperty\n",
      "1 \t gpt identifies wrong propety, and gpt attributes characteristics to sentence which are incorrect\n",
      "10 \t gpt finding things which aren't there\n",
      "1 \t gpt got it right once\n",
      "2 \t gpt is just completely wrong. It's made a soup of the sentence and property definitions\n",
      "2 \t gpt misinterprets the property\n",
      "2 \t gpt misaligned between property it assigns and explanation it gives.\n",
      "3 \t gpt doesn't provide explanation\n",
      "6 \t gpt explanation is wrong\n",
      "1 \t gpt explanation is correct\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Modifying_clauses.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d5dfe-75ea-44d7-bd1e-36177e36d2e5",
   "metadata": {},
   "source": [
    "# Modifying phrases\n",
    "Can be extracted from parse tree. no need for annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fa967-dec9-46ff-98ae-49b5d20647da",
   "metadata": {},
   "source": [
    "# Mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e93f43d3-e093-4d96-bdb8-f1b9c2938107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "human accuracy where all annotators agree: 1.0\n",
      "human accuracy all examples: 0.7241379310344828\n",
      "gpt accuracy all examples: 0.7586206896551724\n",
      "----------------------------------------------------------------------------------------------------\n",
      "28 \t nan\n",
      "1 \t gpt is correct but the explanation is wrong.\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Mood_02.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consitency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2fb59-284e-4c21-9abe-9865beef1595",
   "metadata": {},
   "source": [
    "# New_words_and_changing_uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "bab4269e-d6d9-4d8f-9fa7-bfa812706d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "human accuracy where all annotators agree: 0.8421052631578947\n",
      "human accuracy all examples: 0.72\n",
      "gpt accuracy all examples: 0.16\n",
      "----------------------------------------------------------------------------------------------------\n",
      "7 \t GPT is misinterpreting the property. Using it out of context of grammar.\n",
      "4 \t GPT explanation does not match classification\n",
      "1 \t GPT makde upo its own property\n",
      "1 \t GPT is wrong about the part of speech\n",
      "6 \t nan\n",
      "4 \t GPT is only partially correct\n",
      "2 \t GPT explanation is soup\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/New_words_and_changing_uses.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c9e65-e368-4539-a813-27014ee797ef",
   "metadata": {},
   "source": [
    "# Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "295658a3-8b83-420c-bcaa-d18fa6f084aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "human accuracy where all annotators agree: 0.9545454545454546\n",
      "human accuracy all examples: 0.7096774193548387\n",
      "gpt accuracy all examples: 0.0967741935483871\n",
      "----------------------------------------------------------------------------------------------------\n",
      "21 \t GPT explanation is incorrect\n",
      "10 \t nan\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Parallelism.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3260d5-5a5a-4081-a881-596674a65051",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "GPT assigns the 'grammatical_structure_(parison)' property for almost all examples which is incorrect. Humans correctly assing no property in most cases. The feature is so rarely found it may not be useful. Or, it may be that it is difficult to detect for humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f1474-5708-4c72-a235-d91d3a4d4c1b",
   "metadata": {},
   "source": [
    "# Phrases built on nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c7042a3a-2c7c-4707-a698-c8054080b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "human accuracy where all annotators agree: 1.0\n",
      "human accuracy all examples: 0.9032258064516129\n",
      "gpt accuracy all examples: 0.03225806451612903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "27 \t nan\n",
      "2 \t GPT explanation males no sense\n",
      "2 \t GPT mixes up appositives with summative modifiers\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Phrases_built_on_nouns.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26ca7f-0a44-4e99-b2f3-a2e5b6e6e525",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "GPT assigns the 'appositives' property for almost all examples which is incorrect. The explanations are not logical. There are very few examples where this feature is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58781f-9a63-42fb-b9bc-20ccd4791a53",
   "metadata": {},
   "source": [
    "# Phrases built on verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e6d27979-33d7-4971-85f1-0cde418173a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 0.7222222222222222\n",
      "human accuracy all examples: 0.43333333333333335\n",
      "gpt accuracy all examples: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "17 \t nan\n",
      "13 \t GPT is making stuff up\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Phrases_built_on_verbs.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8646492-6ff7-4ab6-8c77-106514c791e5",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "GPT assign 'participal_phrases' to most examples, and the explanations are illogical. Most examples do not contain this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ccc5f-5d93-4e04-a167-80e0ca0c0b28",
   "metadata": {},
   "source": [
    "# Predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "dc1f7aa8-6053-4545-91ab-69667df03221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 0.9\n",
      "human accuracy all examples: 0.9\n",
      "gpt accuracy all examples: 0.13333333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "6 \t GPT is mixing up the properties and definitions\n",
      "1 \t GPT is misinterpreting a property\n",
      "23 \t nan\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Predication.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a4f9c-5de7-43e4-bc73-46596d021155",
   "metadata": {},
   "source": [
    "## V3\n",
    "\n",
    "* Predication properties are classified individually\n",
    "* GPT3.5 V3 Accuracy:  0.17\n",
    "---------\n",
    "* Predication is ran as a single prompt for all properties. `get_single_gpt_response()`\n",
    "* When asked about 'predication', GPT is interpreting the term outside of the grammar context of active/passive voice\n",
    "\n",
    "* GPT3.5 V3 accuracy: 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc35877-171c-4697-8146-950e7ece3683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62d9ce-687a-45df-9887-2287fb9dab10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb94b8b-bbbf-496d-b2e4-2d82db1db054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63815db5-0f0d-4673-86ac-47e2348a18d7",
   "metadata": {},
   "source": [
    "# Prosody and Punctuation\n",
    "\n",
    "Human annotators did not find any properties of prosody and punctuation in the corpus. This may be another one of those featres that is subjective and nuanced and therefore very difficult for both humans and machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e0f6b-ebc6-43b1-b7fd-c3da11abcf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cff7d5f2-b6af-4700-ada5-4899bd6e56f5",
   "metadata": {},
   "source": [
    "# Sentence architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a2564d4e-1214-4e23-9fee-88a0943839da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 0.9666666666666667\n",
      "human accuracy all examples: 0.9666666666666667\n",
      "gpt accuracy all examples: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "24 \t GPT defaults to right-branching. Explanation is incorrect.\n",
      "1 \t GPT explanation is not logical\n",
      "4 \t GPT misinterpreting the 'loose' feature\n",
      "1 \t GPT misinterpreting the 'periodic' feature\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Sentence_architecture.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f2efb-ef55-42ed-b3d3-3432582b55f6",
   "metadata": {},
   "source": [
    "# Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ead880c7-8c9b-45c5-bd95-19939361dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 1.0\n",
      "human accuracy all examples: 0.9\n",
      "gpt accuracy all examples: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "9 \t GPT misinterprets the feature\n",
      "14 \t nan\n",
      "4 \t GPT explanation is not logical\n",
      "3 \t GPT makes stuff up\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Series.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6efe9-e0ab-4ece-85ac-6d9dcf65fcfc",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "GPT interprets any conjunctions as polysyndenton, and any commas as asyndenton regardless of whether there is a series present or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b2a30-4b3c-47c7-965d-30000cc07ae9",
   "metadata": {},
   "source": [
    "# Subject choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b2fea151-cd21-4054-afdd-692fa37b75d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy where all annotators agree: 0.8666666666666667\n",
      "human accuracy all examples: 0.8666666666666667\n",
      "gpt accuracy all examples: 0.13333333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "5 \t GPT did not adhere to definitions\n",
      "5 \t nan\n",
      "4 \t GPT is mangling the definitions\n",
      "1 \t GPT is misinterpreting the property. Using it out of context of grammar.\n",
      "7 \t GPT explanation is only partially correct\n",
      "6 \t GPT is assinging the poperty to the object not the subject\n",
      "2 \t GPT classification and explanation is correct\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Subject_choices.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e96056-dbc3-40a7-99ad-692b1d2862c6",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "GPT assigns 'rhetorical participants' to the majority of examples. It is also assinging properties to any nouns within the sentence not just the subjects. It's also not sticking to the definitions provided. \n",
    "\n",
    "Tell gpt to identify the subjects first, then assign properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81c665-3abc-463f-acda-e9e24e4e441b",
   "metadata": {},
   "source": [
    "## GPT3.5 - V3\n",
    "* Subject choices are classified individually\n",
    "* Acuuracy: 0.5\n",
    "\n",
    "## GPT4 - V3\n",
    "* Subject choices are classified individually\n",
    "* Acuuracy: 0.47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825ca12-6b6b-4517-ab3f-116a664a8a29",
   "metadata": {},
   "source": [
    "# Tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f93518d6-d7b0-4c8c-b300-9555ca17706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy: 0.7333333333333333\n",
      "gpt accuracy: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "28 \t nan\n",
      "2 \t GPT misinterprets the definition\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Tense.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "print(\"human accuracy:\",sum(verified_df['humans isCorrect'].astype(float))/len(verified_df))\n",
    "print(\"gpt accuracy:\",sum(verified_df['gpt isCorrect'].astype(float))/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73e88c-57d6-43a2-9b53-79740ea67526",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "The text about news headlines being in the present confuses gpt to assign present tense even when the tense is not present.\n",
    "\n",
    "Much of the time GPT is just plain wrong, assining 'present' when it's 'past' and vise versa. This is also seen in the explanations. GPT also tends to assign 'progression' to most examples.\n",
    "\n",
    "What seems strange is that the human annotators are sometimes in full agreement on the wrong tense when the answer is seemingly obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21689ba6-cb0c-490a-92c8-a5475acfee90",
   "metadata": {},
   "source": [
    "# Tropes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7d26d421-af18-4709-886c-0a897302cf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "human accuracy where all annotators agree: 0.7\n",
      "human accuracy all examples: 0.46875\n",
      "gpt accuracy all examples: 0.09375\n",
      "----------------------------------------------------------------------------------------------------\n",
      "31 \t nan\n",
      "1 \t GPT is partially correct\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Tropes.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "# what percentage of the fully agreed examples are correct?\n",
    "\n",
    "total_agree = sum(verified_df['annotator_consistency'])\n",
    "\n",
    "def getNumCorrect(x):\n",
    "    return x['props_a20'] == x['props_a21'] == x['props_a22'] == x['ground truth']\n",
    "\n",
    "verified_df['n_cor'] = verified_df.apply(lambda x: getNumCorrect(x), axis=1)\n",
    "\n",
    "print(\"human accuracy where all annotators agree:\",sum(verified_df['n_cor'])/total_agree)\n",
    "print(\"human accuracy all examples:\",sum(verified_df['humans isCorrect'])/len(verified_df))\n",
    "print(\"gpt accuracy all examples:\",sum(verified_df['gpt isCorrect'])/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed5819-e5f5-4832-85ef-745904bba0de",
   "metadata": {},
   "source": [
    "### Main types of errors\n",
    "\n",
    "This one is clearly hard for both humans and gpt. Probabaly because it can be subjective and very nuanced.\n",
    "\n",
    "GPT's explanations are very poor - neither correct, nor logical.\n",
    "\n",
    "It would be nice to get more examples. Most sentences do not use tropes, and the ones that do, use hyperbole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e192ba-28f3-4c5e-aa1b-e85a4390a5a4",
   "metadata": {},
   "source": [
    "# Verb choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "be4f4293-81f8-406c-bf37-12c776a2e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "human accuracy: 0.7333333333333333\n",
      "gpt accuracy: 0.23333333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15 \t GPT wrongly assings negation\n",
      "15 \t nan\n"
     ]
    }
   ],
   "source": [
    "verified_df = pd.read_csv(\"data/human_gpt_verified/Verb_choices.csv\")\n",
    "verified_df = verified_df[verified_df[\"humans isCorrect\"]>=0]\n",
    "print(len(verified_df))\n",
    "\n",
    "print(\"human accuracy:\",sum(verified_df['humans isCorrect'].astype(float))/len(verified_df))\n",
    "print(\"gpt accuracy:\",sum(verified_df['gpt isCorrect'].astype(float))/len(verified_df))\n",
    "\n",
    "comments = Counter(verified_df['comments'])\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k,v in comments.items():\n",
    "    print(v,\"\\t\",k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc8341-3468-4099-9ab6-82e7143e4b5a",
   "metadata": {},
   "source": [
    "GPT tends to assign negation to everything. THe explanations make up stuff that's not there to justify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a3638c34-5ec2-4d58-9b89-42b35a3a1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_df = pd.read_csv(\"data/human_gpt_verified/all_verified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "41849b6c-6f4c-4832-9bb0-639e21be3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_consistent = all_features_df[all_features_df['annotator_consistency']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "17c94ce5-bdd7-4963-ae9f-ca1180c9bd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "49\n",
      "244\n",
      "361\n"
     ]
    }
   ],
   "source": [
    "print(len(ann_consistent))\n",
    "print(len(ann_consistent[ann_consistent['gpt3.5 isCorrect']==True]))\n",
    "print(len(ann_consistent[ann_consistent['gpt4 isCorrect']==True]))\n",
    "print(len(ann_consistent[ann_consistent['humans isCorrect']==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "fba40d2a-8265-4b8b-833e-358fcf498a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11316397228637413"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT3.5 - where annotator agreement is 100%\n",
    "49/433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "cff46e11-2faf-4676-bdab-b05cb5b1f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5635103926096998"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT4 - where annotator agreement is 100%\n",
    "244/433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "685409e9-e972-41cf-9757-069a07f706e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8337182448036952"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Humans - where annotator agreement is 100%\n",
    "361/433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "23751b5e-917c-4bb2-83fd-efd61d84b9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12903225806451613"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT3.5 - where we have gold labels\n",
    "sum(all_features_df['gpt isCorrect'])/len(all_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "1acabe7f-f539-4833-bde2-5d1d56a412c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053763440860215"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT4 - where we have gold labels\n",
    "sum(all_features_df['gpt4 isCorrect'])/len(all_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "4142df4a-ca31-4cf7-80fe-b64945f5dd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aspect', 'Figures_of_argument', 'Figures_of_word_choice',\n",
       "       'Language_varieties', 'Lexical_and_semantic_fields',\n",
       "       'Modifying_clauses', 'Mood', 'New_words_and_changing_uses',\n",
       "       'Parallelism', 'Phrases_built_on_nouns', 'Phrases_built_on_verbs',\n",
       "       'Predication', 'Sentence_architecture', 'Series',\n",
       "       'Subject_choices', 'Tense', 'Tropes', 'Verb_choices'], dtype=object)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_df['feature_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f39aeca7-8eaf-4240-963a-66fa781e1f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect \t 0.45714285714285713\n",
      "Figures_of_argument \t 0.6666666666666666\n",
      "Figures_of_word_choice \t 0.4\n",
      "Language_varieties \t 0.5434782608695652\n",
      "Lexical_and_semantic_fields \t 0.2\n",
      "Modifying_clauses \t 0.6206896551724138\n",
      "Mood \t 0.4827586206896552\n",
      "New_words_and_changing_uses \t 0.08\n",
      "Parallelism \t 0.8064516129032258\n",
      "Phrases_built_on_nouns \t 0.967741935483871\n",
      "Phrases_built_on_verbs \t 0.6666666666666666\n",
      "Predication \t 0.4\n",
      "Sentence_architecture \t 0.7333333333333333\n",
      "Series \t 0.8333333333333334\n",
      "Subject_choices \t 0.23333333333333334\n",
      "Tense \t 0.4\n",
      "Tropes \t 0.3125\n",
      "Verb_choices \t 0.2\n"
     ]
    }
   ],
   "source": [
    "for f in all_features_df['feature_id'].unique():\n",
    "    df = all_features_df[all_features_df['feature_id']==f]\n",
    "    print(f,\"\\t\",sum(df['gpt4 isCorrect'])/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ffc54-4362-4528-8788-7ea03d64f81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda79924-a20b-4a96-957d-a5c483e3508b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af5802-0774-4083-ab4f-727ad6482e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e10063-da39-43f5-a0e4-a9c8c63a565c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8cd8f-f69b-4771-abbd-9220c362cdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
